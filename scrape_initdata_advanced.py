"""
Script nÃ¢ng cao Ä‘á»ƒ láº¥y initData tá»« Foody.vn
- CÃ³ progress bar
- Auto retry khi fail
- Resume tá»« checkpoint
- Multi-threading option
"""

import requests
import json
import time
import re
from typing import Optional, Dict, Any, List
from datetime import datetime
import os

class FoodyInitDataScraper:
    def __init__(self, checkpoint_file="checkpoint.json", output_file="restaurant_initdata.json"):
        self.checkpoint_file = checkpoint_file
        self.output_file = output_file
        self.session = self._create_session()
        self.results = []
        self.errors = []
        self.processed_urls = set()
        
        # Load checkpoint náº¿u cÃ³
        self._load_checkpoint()
    
    def _create_session(self):
        """Táº¡o session vá»›i headers"""
        session = requests.Session()
        session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "vi-VN,vi;q=0.9,en;q=0.8",
            "Referer": "https://www.foody.vn/",
            "Connection": "keep-alive",
        })
        return session
    
    def _load_checkpoint(self):
        """Load checkpoint Ä‘á»ƒ resume"""
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, "r", encoding="utf-8") as f:
                    checkpoint = json.load(f)
                    self.results = checkpoint.get("results", [])
                    self.errors = checkpoint.get("errors", [])
                    self.processed_urls = set(checkpoint.get("processed_urls", []))
                    print(f"âœ… ÄÃ£ load checkpoint: {len(self.processed_urls)} URLs Ä‘Ã£ xá»­ lÃ½")
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ load checkpoint: {e}")
        
        # Load existing output file náº¿u cÃ³
        elif os.path.exists(self.output_file):
            try:
                with open(self.output_file, "r", encoding="utf-8") as f:
                    self.results = json.load(f)
                    self.processed_urls = {item["url"] for item in self.results}
                    print(f"âœ… ÄÃ£ load output file: {len(self.processed_urls)} URLs Ä‘Ã£ xá»­ lÃ½")
            except Exception as e:
                print(f"âš ï¸  KhÃ´ng thá»ƒ load output file: {e}")
    
    def _save_checkpoint(self):
        """LÆ°u checkpoint"""
        checkpoint = {
            "results": self.results,
            "errors": self.errors,
            "processed_urls": list(self.processed_urls),
            "last_updated": datetime.now().isoformat()
        }
        with open(self.checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(checkpoint, f, ensure_ascii=False, indent=2)
    
    def _extract_initdata(self, html_content: str) -> Optional[Dict[Any, Any]]:
        """TrÃ­ch xuáº¥t initData tá»« HTML"""
        patterns = [
            r'var\s+initData\s*=\s*({.*?});',
            r'window\.initData\s*=\s*({.*?});',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html_content, re.DOTALL)
            if match:
                try:
                    json_str = match.group(1)
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    continue
        
        return None
    
    def scrape_url(self, url: str, max_retries: int = 3) -> Optional[Dict[Any, Any]]:
        """Láº¥y initData tá»« 1 URL vá»›i retry"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                
                if response.status_code == 200:
                    initdata = self._extract_initdata(response.text)
                    return initdata
                elif response.status_code == 429:  # Too many requests
                    print(f"  âš ï¸  Rate limited, Ä‘á»£i 5s...")
                    time.sleep(5)
                    continue
                else:
                    return None
                    
            except requests.exceptions.Timeout:
                if attempt < max_retries - 1:
                    print(f"  â±ï¸  Timeout, thá»­ láº¡i láº§n {attempt + 2}...")
                    time.sleep(2)
                    continue
                return None
            except requests.exceptions.ConnectionError:
                if attempt < max_retries - 1:
                    print(f"  ğŸ”Œ Connection error, thá»­ láº¡i láº§n {attempt + 2}...")
                    time.sleep(3)
                    continue
                return None
            except Exception as e:
                print(f"  âŒ Error: {e}")
                return None
        
        return None
    
    def scrape_all(self, urls: List[str], save_interval: int = 50, delay: float = 1.0):
        """Scrape táº¥t cáº£ URLs"""
        total = len(urls)
        start_time = time.time()
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Báº®T Äáº¦U SCRAPE")
        print(f"ğŸ“Š Tá»•ng sá»‘ URLs: {total}")
        print(f"âœ… ÄÃ£ xá»­ lÃ½: {len(self.processed_urls)}")
        print(f"â³ CÃ²n láº¡i: {total - len(self.processed_urls)}")
        print(f"{'='*60}\n")
        
        for idx, url in enumerate(urls, 1):
            # Skip náº¿u Ä‘Ã£ xá»­ lÃ½
            if url in self.processed_urls:
                continue
            
            # Progress
            elapsed = time.time() - start_time
            avg_time = elapsed / len(self.processed_urls) if self.processed_urls else 0
            remaining = (total - len(self.processed_urls)) * avg_time if avg_time > 0 else 0
            
            print(f"\n[{idx}/{total}] ({len(self.results)} thÃ nh cÃ´ng, {len(self.errors)} lá»—i)")
            print(f"â±ï¸  Thá»i gian: {elapsed/60:.1f}m | CÃ²n láº¡i: ~{remaining/60:.1f}m")
            print(f"ğŸ”— {url}")
            
            # Scrape
            initdata = self.scrape_url(url)
            
            if initdata:
                self.results.append({
                    "url": url,
                    "initData": initdata,
                    "scraped_at": datetime.now().isoformat()
                })
                self.processed_urls.add(url)
                print(f"  âœ… ThÃ nh cÃ´ng!")
            else:
                self.errors.append({
                    "url": url,
                    "index": idx,
                    "attempted_at": datetime.now().isoformat()
                })
                self.processed_urls.add(url)  # ÄÃ¡nh dáº¥u Ä‘Ã£ thá»­ Ä‘á»ƒ khÃ´ng thá»­ láº¡i
                print(f"  âŒ Tháº¥t báº¡i")
            
            # Save checkpoint
            if idx % save_interval == 0:
                print(f"\nğŸ’¾ Äang lÆ°u checkpoint...")
                self._save_checkpoint()
                self._save_results()
            
            # Delay
            time.sleep(delay)
        
        # Save cuá»‘i cÃ¹ng
        print(f"\n{'='*60}")
        print(f"ğŸ’¾ Äang lÆ°u káº¿t quáº£ cuá»‘i cÃ¹ng...")
        self._save_checkpoint()
        self._save_results()
        
        # Thá»‘ng kÃª
        total_time = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"âœ… HOÃ€N THÃ€NH!")
        print(f"{'='*60}")
        print(f"â±ï¸  Tá»•ng thá»i gian: {total_time/60:.1f} phÃºt")
        print(f"âœ… ThÃ nh cÃ´ng: {len(self.results)}/{total}")
        print(f"âŒ Tháº¥t báº¡i: {len(self.errors)}/{total}")
        print(f"ğŸ“Š Tá»‰ lá»‡ thÃ nh cÃ´ng: {len(self.results)/total*100:.1f}%")
        print(f"ğŸ’¾ ÄÃ£ lÆ°u: {self.output_file}")
        if self.errors:
            print(f"ğŸ’¾ Lá»—i: scrape_errors.json")
        print(f"{'='*60}\n")
    
    def _save_results(self):
        """LÆ°u káº¿t quáº£"""
        with open(self.output_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        if self.errors:
            with open("scrape_errors.json", "w", encoding="utf-8") as f:
                json.dump(self.errors, f, ensure_ascii=False, indent=2)

def main():
    # Load URLs
    print("ğŸ“– Äang Ä‘á»c file URLs...")
    with open("final_result_link.json", "r", encoding="utf-8") as f:
        urls = json.load(f)
    
    # Táº¡o scraper
    scraper = FoodyInitDataScraper(
        checkpoint_file="checkpoint.json",
        output_file="restaurant_initdata.json"
    )
    
    # Scrape
    scraper.scrape_all(
        urls=urls,
        save_interval=50,  # LÆ°u sau má»—i 50 URLs
        delay=1.0  # Delay 1 giÃ¢y giá»¯a cÃ¡c requests
    )

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ÄÃ£ dá»«ng bá»Ÿi ngÆ°á»i dÃ¹ng (Ctrl+C)")
        print("ğŸ’¾ Checkpoint Ä‘Ã£ Ä‘Æ°á»£c lÆ°u, cÃ³ thá»ƒ resume sau")
    except Exception as e:
        print(f"\n\nâŒ Lá»—i nghiÃªm trá»ng: {e}")
        raise

